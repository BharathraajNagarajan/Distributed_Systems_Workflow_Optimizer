# Distributed_Systems_Workflow_Optimizer

## ğŸ“Œ Overview
This project demonstrates a **Spark-based distributed systems workflow optimizer** with two major components:
1. **Graph Optimizer (Dijkstraâ€™s Algorithm)** â€“ implemented in PySpark to compute shortest paths across large graphs efficiently.
2. **Fault-tolerant Streaming Pipeline** â€“ built with **Spark Structured Streaming** and checkpointing for real-time word count and recovery from failures.

It combines **batch + streaming workloads** and showcases how Spark can be used for scalable graph analytics and streaming data reliability.

---

## ğŸš€ Features
- âœ… **Dijkstra Graph Optimizer** â€“ computes shortest paths using PySpark RDD/DataFrames.  
- âœ… **Streaming Checkpoint Demo** â€“ ensures recovery after failures with Sparkâ€™s checkpoint mechanism.  
- âœ… **Fault Tolerance** â€“ recovers state automatically via checkpoints (`tmp/chk`).  
- âœ… **Structured Streaming WordCount** â€“ processes new input files in `tmp/stream_in` continuously.  
- âœ… **Colab-Friendly** â€“ runs end-to-end inside Google Colab without external cluster setup.  

---

## ğŸ“‚ Project Structure

Distributed_Systems_Workflow_Optimizer/  
â”œâ”€â”€ data/                # Graph input data (CSV)  
â”œâ”€â”€ out/                 # Streaming output (word counts)  
â”œâ”€â”€ scripts/             # Utility shell scripts for running jobs  
â”œâ”€â”€ src/                 # Core PySpark programs  
â”‚   â”œâ”€â”€ dijkstra_spark.py  
â”‚   â””â”€â”€ streaming_checkpoint_demo.py  
â”œâ”€â”€ tmp/                 # Temporary directory for input/checkpoints  
â”‚   â”œâ”€â”€ stream_in/       # Streaming input files  
â”‚   â””â”€â”€ chk/             # Checkpoint directory  
â”œâ”€â”€ README.md            # Project documentation  
â””â”€â”€ notebook.ipynb       # End-to-end runnable notebook (Colab)  

---

## ğŸ› ï¸ How It Works

### 1. Graph Optimization (Batch Mode)

Run shortest path computation:  

```bash
!python src/dijkstra_spark.py --input data/graph_edges.csv --source A --destination Z
```

### 2. Streaming WordCount (with Checkpointing)

Start the streaming job:

```bash
!python src/streaming_checkpoint_demo.py \
  --inputDir tmp/stream_in \
  --checkpointDir tmp/chk \
  --triggerSec 5
```

Feed streaming input while job is running:

```bash
!echo "hello spark streaming test" > tmp/stream_in/batch1.txt
!echo "spark streaming checkpoint recovery" > tmp/stream_in/batch2.txt
```

- âœ…Spark will process these files in micro-batches and update word counts.
- âœ…Checkpointing ensures recovery if the job crashes and restarts.

## ğŸ“Œ Why This Project is Important

- Shows how distributed graph algorithms (like Dijkstra) scale on big datasets with Spark.  
- Demonstrates fault tolerance in streaming, critical for production pipelines (finance, IoT, logging, fraud detection, etc.).  
- Provides a Colab-ready workflow, lowering barriers for testing Spark-based distributed systems.  

---

## âš ï¸ Difficulties Faced

We encountered multiple real-world debugging challenges:

âŒ **Concurrent Queries Error** â€“ Spark threw `SparkConcurrentModificationException` when multiple queries used the same checkpoint.  

âŒ **Output Modes Mismatch** â€“ Writing to Parquet with *complete mode* caused errors; required using *append mode*.  

âŒ **Colab Limitation** â€“ Couldnâ€™t run two cells in parallel (stream job + data feeding). We solved this by pre-creating input batches before starting the job.  

âŒ **File Path Issues** â€“ Misplaced script files werenâ€™t included in the zip until we confirmed proper saving in  
`/content/Distributed_Systems_Workflow_Optimizer/scripts`.  

âŒ **Git Push Authentication** â€“ Faced credential issues in Colab; solved by downloading locally and pushing manually.  

â¡ï¸ These reflect the practical hurdles in distributed systems, reinforcing concepts beyond just code.  

---

## ğŸ“‘ Future Improvements

- Add Kafka source for real-time streaming instead of file-based.  
- Extend graph optimizer to handle weighted directed acyclic graphs and multi-source shortest paths.  
- Build a Dockerfile for easier reproducibility outside Colab.


ğŸ‘¤ Author
Bharathraaj Nagarajan

